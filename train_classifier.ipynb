{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/wangc21/datasets/pool/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, label_path, transform = None):\n",
    "        self.labels = pd.read_csv(label_path, sep = \",\", header = None).values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx, label = self.labels[idx][0], self.labels[idx][1]\n",
    "        image = cv2.imread(os.path.join(DATA_PATH, img_idx))\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12824\n",
      "3176\n"
     ]
    }
   ],
   "source": [
    "# data augmentation\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees = 360),\n",
    "    transforms.ColorJitter(brightness = 0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "data_train = PoolDataset(os.path.join(DATA_PATH, \"train_labels.csv\"), transform = train_transforms)\n",
    "print(len(data_train))\n",
    "data_test = PoolDataset(os.path.join(DATA_PATH, \"test_labels.csv\"), transform = test_transforms)\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "        self.fc = nn.Linear(20 * 20 * 64, 16)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # HWC to CHW\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        output = F.log_softmax(x, dim = 1)\n",
    "        return output\n",
    "    \n",
    "    def loss(self, prediction, label):\n",
    "        loss = F.cross_entropy(prediction, label)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    # stochastic gradient descent\n",
    "    for i, (data, label) in enumerate(train_loader):\n",
    "        data = data.to(device = device, dtype = torch.float)\n",
    "        label = label.to(device = device, dtype = torch.long)\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # make prediction\n",
    "        output = model(data)\n",
    "        \n",
    "        # compute error gradients\n",
    "        loss = model.loss(output, label)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print loss every 8 batch iterations\n",
    "        if i % 8 == 0:\n",
    "            print(\"Batch Iteration %d, Train Loss: %.6f\" % (i, loss.item()))\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for (data, label) in test_loader:\n",
    "            data = data.to(device = device, dtype = torch.float)\n",
    "            label = label.to(device = device, dtype = torch.long)\n",
    "            output = model(data)\n",
    "            loss = model.loss(output, label)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            pred = output.argmax(dim = 1, keepdim = True)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "            \n",
    "    test_loss = np.mean(losses)\n",
    "\n",
    "    print(\"\\nTest Loss: {}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return losses, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n",
      "Epoch 1\n",
      "Batch Iteration 0, Train Loss: 2.930107\n",
      "Batch Iteration 8, Train Loss: 1.523392\n",
      "Batch Iteration 16, Train Loss: 0.425550\n",
      "Batch Iteration 24, Train Loss: 0.140156\n",
      "\n",
      "Test Loss: 0.10867239419106056, Accuracy: 3076/3176 (97%)\n",
      "\n",
      "Epoch 2\n",
      "Batch Iteration 0, Train Loss: 0.085204\n",
      "Batch Iteration 8, Train Loss: 0.032347\n",
      "Batch Iteration 16, Train Loss: 0.053543\n",
      "Batch Iteration 24, Train Loss: 0.017421\n",
      "\n",
      "Test Loss: 0.015406847219679776, Accuracy: 3161/3176 (100%)\n",
      "\n",
      "Epoch 3\n",
      "Batch Iteration 0, Train Loss: 0.019111\n",
      "Batch Iteration 8, Train Loss: 0.024411\n",
      "Batch Iteration 16, Train Loss: 0.012990\n",
      "Batch Iteration 24, Train Loss: 0.015293\n",
      "\n",
      "Test Loss: 0.009406624172811462, Accuracy: 3165/3176 (100%)\n",
      "\n",
      "Epoch 4\n",
      "Batch Iteration 0, Train Loss: 0.013533\n",
      "Batch Iteration 8, Train Loss: 0.006982\n",
      "Batch Iteration 16, Train Loss: 0.011955\n",
      "Batch Iteration 24, Train Loss: 0.007486\n",
      "\n",
      "Test Loss: 0.008390863385400849, Accuracy: 3165/3176 (100%)\n",
      "\n",
      "Epoch 5\n",
      "Batch Iteration 0, Train Loss: 0.014426\n",
      "Batch Iteration 8, Train Loss: 0.024030\n",
      "Batch Iteration 16, Train Loss: 0.007451\n",
      "Batch Iteration 24, Train Loss: 0.005256\n",
      "\n",
      "Test Loss: 0.004770898276802882, Accuracy: 3166/3176 (100%)\n",
      "\n",
      "Epoch 6\n",
      "Batch Iteration 0, Train Loss: 0.003432\n",
      "Batch Iteration 8, Train Loss: 0.005611\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ec9ccc28985f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtrain_curve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-9a0318080c77>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# stochastic gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "EPOCHS = 20\n",
    "TRAIN_BATCH_SIZE = 400\n",
    "TEST_BATCH_SIZE = 99\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "# hardware device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Using: %s\" % device)\n",
    "kwargs = {'num_workers': 0,\n",
    "          'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# data loaders\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size = TRAIN_BATCH_SIZE,\n",
    "                                           shuffle = True, drop_last = True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size = TEST_BATCH_SIZE,\n",
    "                                          shuffle = False, drop_last = True, **kwargs)  \n",
    "\n",
    "# training objects\n",
    "model = ConvNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "# training loop\n",
    "train_curve = []\n",
    "test_curve = []\n",
    "best_loss = np.inf\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_losses = train(model, device, train_loader, optimizer, epoch)\n",
    "    test_losses, test_loss = test(model, device, test_loader)\n",
    "    train_curve.extend(train_losses)\n",
    "    test_curve.extend(test_losses)\n",
    "    \n",
    "    # save best model\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.save(model.state_dict(), \"checkpoints/epoch_%d.pt\" % epoch)\n",
    "        \n",
    "# plot loss functions\n",
    "plt.plot(np.arange(EPOCHS * 32) + 1, train_curve, label = \"train\")\n",
    "plt.plot(np.arange(EPOCHS * 32) + 1, test_curve, label = \"test\")\n",
    "plt.legend()\n",
    "plt.xlabel('batch iterations')\n",
    "plt.ylabel('mean loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
